<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
    content="Hidden in Plain Sight: Probing Implicit Reasoning in Multimodal Language Models">">
  <meta name="keywords" content="iReason, Implicit Reasoning, Multimodal LLM">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models</title>

  <link rel="icon" href="./static/images/mathvista.png">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/leaderboard.css">

  <!-- <link href="https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator_bulma.min.css" rel="stylesheet">
  <script type="text/javascript" src="https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js"></script> -->
  <script type="text/javascript" src="static/js/sort-table.js" defer></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/explorer-index.js"></script>
  <script src="./static/js/question_card.js"></script>

  <script src="./static/js/leaderboard_testmini.js"></script>  
  <script src="./data/results/output_folders.js" defer></script>
  <script src="./data/results/model_scores.js" defer></script>

  <script src="./visualizer/data/data_public.js" defer></script>
</head>
<body>

<!-- Heading -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Hidden in Plain Sight: Probing Implicit Reasoning in Multimodal Language Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://Jackie-2000.github.io/">Qianqi Yan</a><sup style="color:#6fbf73;">1</sup>,</span>
            <span class="author-block">
              Hongquan Li,
            </span>
            <span class="author-block">
              Shan Jiang<sup style="color:#ed4b82">2</sup>,
            </span>
            <span class="author-block">
              Yang Zhao<sup style="color:#ed4b82">2</sup>,
            </span>
            <span class="author-block">
              Xinze Guan<sup style="color:#ed4b82">2</sup>,
            </span>
            <span class="author-block">
              Ching-Chen Kuo<sup style="color:#ed4b82">2</sup>,
            </span>
            <span class="author-block">
              <a href="https://eric-xw.github.io/">Xin Eric Wang</a><sup style="color:#6fbf73;">1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup style="color:#6fbf73;">1</sup>University of California, Santa Cruz,</span><br>
            <span class="author-block"><sup style="color:#ed4b82">2</sup>eBay</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/xxx"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/xxx"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/eric-ai-lab/iReason"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/rippleripple/iReason"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      <p style="font-size:18px">ü§ó</p>
                      <!-- üîó -->
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser figures -->
<section class="section">
  <div class="container" style="margin-top: -150px; margin-bottom: -100px;">
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/teaser.png" alt="teaser" width="80%"/>
              <p> 
                Even when the instruction appears valid, it may silently conflict with the visual context. Implicit reasoning requires models to detect whats missing, ambiguous, contradictory, or infeasible‚Äîwithout being told.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
</section>

<!-- Intro -->
<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            Multimodal Large Language Models (MLLMs) are increasingly deployed in open-ended, real-world environments. Real instructions often involve <b>missing objects, ambiguous references, contradictory facts, or infeasible tasks</b>‚Äîsituations that require <b>implicit reasoning</b> beyond simple execution. Existing benchmarks mostly assume that the visual input and instruction are perfectly aligned, overlooking cases where flaws must be inferred from context. This paper provides a systematic analysis of how current MLLMs handle implicit reasoning scenarios where the problem is ‚Äúhidden in plain sight.‚Äù
          </p>
          
          <b>We organize our analysis around three key research questions (RQs):</b>
          <ul>
            <li>
              <b>RQ1:</b> How do MLLMs perform on implicit reasoning tasks?
            </li>
            <li>
              <b>RQ2:</b> Do models recognize hidden issues internally but suppress them, or is failure due to lack of ability?
            </li>
            <li>
              <b>RQ3:</b> Can simple inference-time interventions (e.g., clarifying questions) recover suppressed reasoning and improve trustworthiness?
            </li>
          </ul>
          
          <p>
            We curate a diagnostic suite covering four real-world failure modes and evaluate six leading MLLMs, including o3 and GPT-4o, on 643 diverse samples. 
          </p>
          <!-- table: stats -->
          <div id="results-carousel" class="carousel results-carousel">
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/examples.png" alt="examples" width="50%"/>
                <p> There are four categories under the implicit reasoning scenarios, posing diverse challenges.
                </p>
              </div>
            </div>
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/stats.png" alt="stats" width="80%"/>
                <p> iReason Statistics. Breakdown of the testbed by category. Please see paper Appendix for detailed data curation.
                </p>
              </div>
            </div>
          </div>
         
          <b>Main Findings:</b>
          <ul>
            <li>Even advanced models frequently fail to surface hidden issues, despite possessing the necessary perceptual and reasoning skills.</li>
            <li>Explicit prompting and clarifying question interventions can dramatically recover performance, revealing a gap between competence and compliance.</li>
            <li>Our analysis highlights the need to address behavioral alignment in addition to model capability for trustworthy AI.</li>
          </ul>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</div>
</section>

<!-- RQ1: How do MLLMs Perform on Implicit Reasoning Tasks? -->
<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">RQ1: How do MLLMs Perform on Implicit Reasoning Tasks?</h2>
        <div class="content has-text-justified">
          <ul>
            <li>
              <b>Setup:</b> We test six state-of-the-art MLLMs on a diagnostic suite spanning four implicit reasoning challenges:
              <ul>
                <li><b>Object Absence:</b> The referenced entity is missing.</li>
                <li><b>Referential Ambiguity:</b> Multiple plausible targets exist.</li>
                <li><b>Factual Contradiction:</b> Key facts in the scene disagree.</li>
                <li><b>Goal Infeasibility:</b> The requested plan is physically, temporally, or causally impossible or unsafe.</li>
              </ul>
            </li>
            <li>
              <b>Evaluation:</b> Each model receives a single image and task string, with no extra hints. The model must return a free-form answer; an LLM judge determines correctness.
            </li>
            <li>
              <b>Key Findings:</b>
              <ul>
                <li>Even the strongest models (o3, GPT-4o) detect <b>less than 40%</b> of implicit issues in default settings.</li>
                <li>Open-source models usually fall below 20% accuracy.</li>
                <li>Some categories (like Referential Ambiguity) are more tractable, but Goal Infeasibility is extremely challenging for all models.</li>
              </ul>
            </li>
            <li>
              <b>Interpretation:</b> Current MLLMs remain vulnerable to hidden, real-world errors that require reading between the lines rather than just task execution.
            </li>
          </ul>
          <!-- figures -->
          <div id="results-carousel" class="carousel results-carousel">
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/main.png" alt="grade-lv" width="50%"/>
                <p>
                  The accuracy (%) of six MLLMs under the four categories. Proprietary models demonstrate higher performance. The best result in each question category is in-bold, and the second best is underlined.
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- RQ2: Do Models Know More Than They Say? -->
<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">RQ2: Do Models Know More Than They Say?</h2>
        <div class="content has-text-justified">
          <ul>
            <li>
              <b>Question:</b> When models fail at implicit reasoning, is it due to a lack of capability‚Äîor do they recognize issues internally but fail to express them?
            </li>
            <li>
              <b>Experiments:</b>
              <ul>
                <li>
                  <b>Explicit Prompts:</b> We ask direct yes/no questions (e.g., "Is the referenced object present?") for the same samples.
                </li>
                <li>
                  <b>Chain-of-Thought (CoT):</b> We prompt models to "think step by step," then separately score the internal reasoning trace and final answer.
                </li>
              </ul>
            </li>
            <li>
              <b>Key Findings:</b>
              <ul>
                <li>With explicit prompts, top models achieve <b>83%+</b> accuracy‚Äîdemonstrating the necessary skills exist.</li>
                <li>CoT traces show that proprietary models (o3, GPT-4o) often recognize flaws internally, but omit them in their final answer‚Äîindicating suppression due to alignment or obedience pressure.</li>
                <li>Open-source models generally fail both internally and externally, showing true reasoning limitations.</li>
              </ul>
            </li>
            <li>
              <b>Interpretation:</b> There is a critical gap between internal competence and actual model behavior. For strong models, failures often reflect ‚Äúbehavioral alignment‚Äù rather than lack of reasoning.
            </li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- RQ3: Can Inference-Time Interventions Recover Reasoning? -->
<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">RQ3: Can Inference-Time Interventions Recover Reasoning?</h2>
        <div class="content has-text-justified">
          <ul>
            <li>
              <b>Question:</b> If the models already ‚Äúknow‚Äù the issue, can we unlock their suppressed reasoning with simple interventions‚Äîwithout retraining?
            </li>
            <li>
              <b>Interventions Tested:</b>
              <ul>
                <li>
                  <b>System Persona Prompting:</b> Adding a prompt that encourages the model to prioritize caution, honesty, and factual correctness over obedience.
                </li>
                <li>
                  <b>Interactive Clarification:</b> Allowing‚Äîor requiring‚Äîthe model to ask the user a clarifying question before answering.
                </li>
              </ul>
            </li>
            <li>
              <b>Key Findings:</b>
              <ul>
                <li>System persona prompts yield modest performance gains, especially for strong models.</li>
                <li>Allowing models to ask clarifying questions significantly boosts performance for all models, but especially for open-source ones.</li>
                <li>Requiring a clarifying question (forcing the model to ask) leads to <b>over 94% accuracy</b> for o3 and GPT-4o‚Äîfully recovering their suppressed competence.</li>
              </ul>
            </li>
            <li>
              <b>Interpretation:</b> Simple prompt-level interventions are highly effective for bridging the gap between what models know and what they say‚Äîpointing to actionable ways to increase trust and safety in real-world applications.
            </li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <div class="column">
            Our comprehensive evaluation of six state-of-the-art multimodal models (including proprietary systems like <b>o1</b> and <b>GPT-4o</b>, and open-source models like <b>Qwen2.5-VL, LLaVA-NeXT, InternVL2.5</b> and <b>Phi-3.5-Vision</b>) reveals critical insights into multimodal inconsistency reasoning. Proprietary models significantly outperform open-source models, with o1 achieving over 50% accuracy overall‚Äîsurpassing open-source models by more than 30%. While all models struggle with complex inconsistencies, proprietary systems show stronger alignment between visual and textual reasoning, particularly when provided with contextual cues.

            <div class="content">
              <p class="mt-3">
                The accuracy of six MLLMs under the two evaluation settings. Proprietary models demonstrate higher performance as well as larger performance gain in the MCQ setting. While MCQ-style prompts boost GPT-4o's accuracy by ~15%, open-source models gain minimal benefits, highlighting fundamental reasoning gaps.
              </p>
            
              <div>
                <p>üö® To submit your results to the leaderboard, please send to <a href="mailto:qyan79@ucsc.edu">this email</a> with your result JSON files.</p>
                <p>üö® For more submission details, please refer to <a href="https://github.com/eric-ai-lab/MMIR">this link.</a></p>
              </div>
              
              <div>
              <h3 class="title is-4 mt-4">Fine-grained error analysis</h3>
              <ul>
                <li><b>Performance Gap:</b> Proprietary models excel at detecting <i>factual contradictions</i> and <i>identity mismatches</i>, but even top models like GPT-4o show limitations in resolving <i>temporal/spatial incoherence</i>.</li>
                <li><b>Modality Matters:</b> Models handle text-text inconsistencies best but falter with image-image comparisons, exposing weaknesses in visual reasoning.</li>
                <li><b>Layout Complexity:</b> Performance drops sharply as artifacts become visually dense‚Äîmodels lose up to 40% accuracy on cluttered layouts compared to simple ones.</li>
              </ul>
              </div>

              <!-- figures -->
              <div id="results-carousel" class="carousel results-carousel">
                <div class="box m-5">
                  <div class="content has-text-centered">
                    <img src="static/images/ablation_error.png" alt="grade-lv" width="80%"/>
                    <p>
                      Fine-grained analysis of model performance across Inconsistency Categories and Modalities.
                    </p>
                  </div>
                </div>
                <div class="box m-5">
                  <div class="content has-text-centered">
                    <img src="static/images/ablation_count.png" alt="grade-lv" width="60%"/>
                    <p>
                      Model performance on layout complexity.
                    </p>
                  </div>
                </div>
              </div>

              <h3 class="title is-4 mt-4">Prompting Strategies Analysis</h3>
              To enhance multimodal inconsistency reasoning, we tested three prompting approaches and has the following observations:
              <ul>
                <li><b>Chain-of-Thought (CoT):</b> Explicit textual reasoning steps provided minimal benefits, sometimes reducing accuracy, especially for open-source models.</li>
                <li><b>Set-of-Mark (SoM):</b> Visual bounding boxes improved GPT-4o‚Äôs performance (+5%) but confused other models, often degrading results.</li>
                <li><b>Multimodal Interleaved CoT (MM-CoT):</b> Our novel two-stage method combined textual reasoning with iterative visual refinement.</li>
              </ul>
              
              <div id="results-carousel" class="carousel results-carousel">
                <div class="box m-5">
                  <div class="content has-text-centered">
                    <img src="static/images/probing.png" alt="probing" width="60%"/>
                    <p> <b>Probing results of different prompting methods.</b> Performance of each prompting method is directly compared with the vanilla setting. Gains are in blue and drops are in red.
                  </div>
                </div>
              </div>
              <p>
                MM-CoT outperformed all other methods, boosting GPT-4o's accuracy by 4.4% and showing modest gains for open-source models.
                Proprietary models benefited most from iterative cross-modal integration, while isolated prompts (CoT/SoM) proved ineffective.
                Visual annotations only helped when guided by initial textual reasoning, highlighting the need for tightly coupled multimodal interaction.
              </p>
            </div>
          </div>
      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3 has-text-centered">BibTeX</h2>
    <pre><code>
      @misc{yan2025multimodalinconsistencyreasoningmmir,
        title={Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models}, 
        author={Qianqi Yan and Yue Fan and Hongquan Li and Shan Jiang and Yang Zhao and Xinze Guan and Ching-Chen Kuo and Xin Eric Wang},
        year={2025},
        eprint={2502.16033},
        archivePrefix={arXiv},
        primaryClass={cs.AI},
        url={https://arxiv.org/abs/2502.16033}, 
  }
    </code></pre>
  </div>
</section>


<footer class="footer">
  <!-- <div class="container"> -->
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is website adapted from <a href="https://nerfies.github.io/">Nerfies</a>, and <a href="https://mathvista.github.io/">MathVista</a>licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  <!-- </div> -->
</footer>

</body>
</html>
